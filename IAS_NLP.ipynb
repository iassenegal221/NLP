{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IAS NLP",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2pHgELjimiG"
      },
      "source": [
        "$\\textbf{Institut des Algorithmes du Sénégal}$\n",
        "\n",
        "$\\textbf{Introduction to Natural Langage Preprocessing (NLP)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ_YfS_Gi2gY"
      },
      "source": [
        "\n",
        "\n",
        "This notebook contains functions required for text cleaning and processing pipeline in NLP problems. These are ready-to-use functions and use NLTK and SKlearn packages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G610XRgri7hG"
      },
      "source": [
        "$\\textbf{Import utils functions}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4lDhGBKiB_8",
        "outputId": "e12c603f-92ff-45c0-95cb-16941a5b1543"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import string as st\n",
        "import re\n",
        "from nltk import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8KkFggsjDnx"
      },
      "source": [
        "$\\textbf{Read the data. }$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "b80BoP47jEVW",
        "outputId": "3292d066-71f7-44a6-acb0-fa590eed613f"
      },
      "source": [
        "data = pd.read_csv('spam_data.csv')\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category                                            Message\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhKX9fgBjhL6",
        "outputId": "99478655-ba7a-4070-ac64-bffb0a4acfa9"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlJ1pWsyjp4t"
      },
      "source": [
        "\n",
        "$\\textbf{Text cleaning and processing steps}$\n",
        "\n",
        "    - Remove punctuations\n",
        "    - Convert text to tokens\n",
        "    - Remove tokens of length less than or equal to 3\n",
        "    - Remove stopwords using NLTK corpus stopwords list to match\n",
        "    - Apply stemming\n",
        "    - Apply lemmatization\n",
        "    - Convert words to feature vectors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t8VWYkojn2M"
      },
      "source": [
        "# Remove all punctuations from the text\n",
        "\n",
        "def remove_punct(text):\n",
        "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3UzAHXZBj82k",
        "outputId": "31349f98-2d7f-4fff-f7af-4044c36aea9d"
      },
      "source": [
        "data['removed_punc'] = data['Message'].apply(lambda x: remove_punct(x))\n",
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "      <th>removed_punc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category  ...                                       removed_punc\n",
              "0      ham  ...  Go until jurong point crazy Available only in ...\n",
              "1      ham  ...                            Ok lar Joking wif u oni\n",
              "2     spam  ...  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  ...        U dun say so early hor U c already then say\n",
              "4      ham  ...  Nah I dont think he goes to usf he lives aroun...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sGimwd1j_sE"
      },
      "source": [
        "''' Convert text to lower case tokens. Here, split() is applied on white-spaces. But, it could be applied\n",
        "    on special characters, tabs or any other string based on which text is to be seperated into tokens.\n",
        "'''\n",
        "def tokenize(text):\n",
        "    text = re.split('\\s+' ,text)\n",
        "    return [x.lower() for x in text]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "PTts_XmLkD92",
        "outputId": "a6eb1b4b-4f94-44f5-cc0c-45b52eafe0c7"
      },
      "source": [
        "data['tokens'] = data['removed_punc'].apply(lambda msg : tokenize(msg))\n",
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "      <th>removed_punc</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category  ...                                             tokens\n",
              "0      ham  ...  [go, until, jurong, point, crazy, available, o...\n",
              "1      ham  ...                     [ok, lar, joking, wif, u, oni]\n",
              "2     spam  ...  [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
              "3      ham  ...  [u, dun, say, so, early, hor, u, c, already, t...\n",
              "4      ham  ...  [nah, i, dont, think, he, goes, to, usf, he, l...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGPL38pDkF_O"
      },
      "source": [
        "# Remove tokens of length less than 3\n",
        "def remove_small_words(text):\n",
        "    return [x for x in text if len(x) > 3 ]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "-n7IHw1EkJu9",
        "outputId": "aa2f741f-95aa-41f5-ae52-5d74b8db2798"
      },
      "source": [
        "data['larger_tokens'] = data['tokens'].apply(lambda x : remove_small_words(x))\n",
        "data.head()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "      <th>removed_punc</th>\n",
              "      <th>tokens</th>\n",
              "      <th>larger_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
              "      <td>[until, jurong, point, crazy, available, only,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
              "      <td>[joking]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, wkly, comp, final, tkts, 21st, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
              "      <td>[early, already, then]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[dont, think, goes, lives, around, here, though]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category  ...                                      larger_tokens\n",
              "0      ham  ...  [until, jurong, point, crazy, available, only,...\n",
              "1      ham  ...                                           [joking]\n",
              "2     spam  ...  [free, entry, wkly, comp, final, tkts, 21st, 2...\n",
              "3      ham  ...                             [early, already, then]\n",
              "4      ham  ...   [dont, think, goes, lives, around, here, though]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMa1NdmakNOc"
      },
      "source": [
        "''' Remove stopwords. Here, NLTK corpus list is used for a match. However, a customized user-defined \n",
        "    list could be created and used to limit the matches in input text. \n",
        "'''\n",
        "def remove_stopwords(text):\n",
        "    return [word for word in text if word not in nltk.corpus.stopwords.words('english')]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "efVoqvfckP_V",
        "outputId": "a3ff0ad9-009b-4ecb-f041-696d56f0bc30"
      },
      "source": [
        "data['clean_tokens'] = data['larger_tokens'].apply(lambda x : remove_stopwords(x))\n",
        "data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "      <th>removed_punc</th>\n",
              "      <th>tokens</th>\n",
              "      <th>larger_tokens</th>\n",
              "      <th>clean_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
              "      <td>[until, jurong, point, crazy, available, only,...</td>\n",
              "      <td>[jurong, point, crazy, available, bugis, great...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
              "      <td>[joking]</td>\n",
              "      <td>[joking]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, wkly, comp, final, tkts, 21st, 2...</td>\n",
              "      <td>[free, entry, wkly, comp, final, tkts, 21st, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
              "      <td>[early, already, then]</td>\n",
              "      <td>[early, already]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[dont, think, goes, lives, around, here, though]</td>\n",
              "      <td>[dont, think, goes, lives, around, though]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category  ...                                       clean_tokens\n",
              "0      ham  ...  [jurong, point, crazy, available, bugis, great...\n",
              "1      ham  ...                                           [joking]\n",
              "2     spam  ...  [free, entry, wkly, comp, final, tkts, 21st, 2...\n",
              "3      ham  ...                                   [early, already]\n",
              "4      ham  ...         [dont, think, goes, lives, around, though]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPa6rlmWkeXk"
      },
      "source": [
        "Apply stemming to convert tokens to their root form. This is a rule-based process of word form conversion where word-suffixes are truncated irrespective of whether the root word is an actual word in the language dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUjRIfjKklgV"
      },
      "source": [
        "Note that this step is optional and depends on problem type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al_vNS5tkTUd"
      },
      "source": [
        "# Apply stemming to get root words \n",
        "def stemming(text):\n",
        "    ps = PorterStemmer()\n",
        "    return [ps.stem(word) for word in text]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "p2hHVJJcknOE",
        "outputId": "0b67dcdf-ab38-413c-e7d2-6969dbe274c9"
      },
      "source": [
        "data['stem_words'] = data['clean_tokens'].apply(lambda wrd: stemming(wrd))\n",
        "data.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "      <th>removed_punc</th>\n",
              "      <th>tokens</th>\n",
              "      <th>larger_tokens</th>\n",
              "      <th>clean_tokens</th>\n",
              "      <th>stem_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
              "      <td>[until, jurong, point, crazy, available, only,...</td>\n",
              "      <td>[jurong, point, crazy, available, bugis, great...</td>\n",
              "      <td>[jurong, point, crazi, avail, bugi, great, wor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
              "      <td>[joking]</td>\n",
              "      <td>[joking]</td>\n",
              "      <td>[joke]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, wkly, comp, final, tkts, 21st, 2...</td>\n",
              "      <td>[free, entry, wkly, comp, final, tkts, 21st, 2...</td>\n",
              "      <td>[free, entri, wkli, comp, final, tkt, 21st, 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
              "      <td>[early, already, then]</td>\n",
              "      <td>[early, already]</td>\n",
              "      <td>[earli, alreadi]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[dont, think, goes, lives, around, here, though]</td>\n",
              "      <td>[dont, think, goes, lives, around, though]</td>\n",
              "      <td>[dont, think, goe, live, around, though]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category  ...                                         stem_words\n",
              "0      ham  ...  [jurong, point, crazi, avail, bugi, great, wor...\n",
              "1      ham  ...                                             [joke]\n",
              "2     spam  ...  [free, entri, wkli, comp, final, tkt, 21st, 20...\n",
              "3      ham  ...                                   [earli, alreadi]\n",
              "4      ham  ...           [dont, think, goe, live, around, though]\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ-Wuhq6ks1l"
      },
      "source": [
        "Lemmatization converts word to it's dictionary base form. This process takes language grammar and vocabulary into consideration while conversion. Hence, it is different from Stemming in that it does not merely truncate the suffixes to get the root word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8gj_U-_kqLU"
      },
      "source": [
        "# Apply lemmatization on tokens\n",
        "def lemmatize(text):\n",
        "    word_net = WordNetLemmatizer()\n",
        "    return [word_net.lemmatize(word) for word in text]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "GaIGMTMMkv3l",
        "outputId": "eeb7ad84-763d-4c78-d05f-32a488d258b7"
      },
      "source": [
        "data['lemma_words'] = data['clean_tokens'].apply(lambda x : lemmatize(x))\n",
        "data.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "      <th>removed_punc</th>\n",
              "      <th>tokens</th>\n",
              "      <th>larger_tokens</th>\n",
              "      <th>clean_tokens</th>\n",
              "      <th>stem_words</th>\n",
              "      <th>lemma_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
              "      <td>[until, jurong, point, crazy, available, only,...</td>\n",
              "      <td>[jurong, point, crazy, available, bugis, great...</td>\n",
              "      <td>[jurong, point, crazi, avail, bugi, great, wor...</td>\n",
              "      <td>[jurong, point, crazy, available, bugis, great...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
              "      <td>[joking]</td>\n",
              "      <td>[joking]</td>\n",
              "      <td>[joke]</td>\n",
              "      <td>[joking]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
              "      <td>[free, entry, wkly, comp, final, tkts, 21st, 2...</td>\n",
              "      <td>[free, entry, wkly, comp, final, tkts, 21st, 2...</td>\n",
              "      <td>[free, entri, wkli, comp, final, tkt, 21st, 20...</td>\n",
              "      <td>[free, entry, wkly, comp, final, tkts, 21st, 2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
              "      <td>[early, already, then]</td>\n",
              "      <td>[early, already]</td>\n",
              "      <td>[earli, alreadi]</td>\n",
              "      <td>[early, already]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[dont, think, goes, lives, around, here, though]</td>\n",
              "      <td>[dont, think, goes, lives, around, though]</td>\n",
              "      <td>[dont, think, goe, live, around, though]</td>\n",
              "      <td>[dont, think, go, life, around, though]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category  ...                                        lemma_words\n",
              "0      ham  ...  [jurong, point, crazy, available, bugis, great...\n",
              "1      ham  ...                                           [joking]\n",
              "2     spam  ...  [free, entry, wkly, comp, final, tkts, 21st, 2...\n",
              "3      ham  ...                                   [early, already]\n",
              "4      ham  ...            [dont, think, go, life, around, though]\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}